import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"
import sys
from dotenv import load_dotenv
# 1. åŠ è½½æ–‡æ¡£çš„å·¥å…·
from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader
# 2. åˆ‡åˆ†æ–‡æœ¬çš„å·¥å…·
from langchain.text_splitter import RecursiveCharacterTextSplitter
# 3. å‘é‡æ•°æ®åº“
from langchain_community.vectorstores import Chroma
# 4. å‘é‡åŒ–æ¨¡å‹ (ç”¨æ¥æŠŠæ–‡å­—å˜æˆæ•°å­—)
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
# åŠ è½½ç¯å¢ƒå˜é‡ (API Key)
load_dotenv()

# --- é…ç½®è·¯å¾„ ---
DATA_PATH = "./data"  # å­˜æ”¾ PDF çš„æ–‡ä»¶å¤¹
CHROMA_PATH = "./chroma_db"  # å­˜æ”¾å‘é‡æ•°æ®åº“çš„æ–‡ä»¶å¤¹ (è‡ªåŠ¨ç”Ÿæˆ)
embeddings = HuggingFaceEmbeddings(
        model_name="/Users/jiaqing/rag/åŸºäº RAG çš„é›†æˆç”µè·¯ä¸“ä¸šçŸ¥è¯†åº“åŠ©æ‰‹/model/m3e-base",
        model_kwargs={'device': 'mps'}  # å…³é”®ï¼šè¿™è¡Œä»£ç èƒ½è°ƒç”¨ä½  Mac çš„ GPU/NPU åŠ é€Ÿ
    )
llm = ChatOpenAI(
    model="deepseek-r1-0528",  # æˆ–è€… moonshot-v1-8k
    temperature=0.1,  # è®¾ä½ä¸€ç‚¹ï¼Œè®©å®ƒå›ç­”æ›´ä¸¥è°¨ï¼Œä¸è¦çç¼–
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_API_BASE")
)
def create_vector_db():
    """
    æ ¸å¿ƒé€»è¾‘ï¼šå¼ºåˆ¶é‡å»ºæ•°æ®åº“ (åŠ è½½ PDF -> åˆ‡åˆ† -> å‘é‡åŒ– -> å­˜å…¥æ•°æ®åº“)
    """
    # --- 1. æ¸…ç†æ—§æˆ˜åœº ---
    # æ— è®ºæ—§æ•°æ®åº“æ˜¯ä¸æ˜¯å­˜åœ¨ï¼Œå…ˆæ£€æŸ¥ä¸€ä¸‹ã€‚å¦‚æœæœ‰ï¼Œç›´æ¥åˆ æ‰ï¼
    # è¿™æ ·èƒ½ä¿è¯æ¯æ¬¡è¿è¡Œéƒ½æ˜¯â€œå…¨æ–°â€çš„ï¼Œä¸ä¼šæ··å…¥æ—§æ–‡æ¡£ã€‚
    if os.path.exists(CHROMA_PATH):
        import shutil
        shutil.rmtree(CHROMA_PATH)
        print(f"ğŸ—‘ï¸ å·²åˆ é™¤æ—§æ•°æ®åº“ {CHROMA_PATH}ï¼Œæ­£åœ¨å‡†å¤‡é‡å»º...")

    # --- 2. æ£€æŸ¥åŸæ–™ ---
    if not os.path.exists(DATA_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ° {DATA_PATH} æ–‡ä»¶å¤¹")
        return None

    # --- 3. åŠ è½½æ–°æ–‡æ¡£ ---
    print("ğŸ”„ 1. æ­£åœ¨åŠ è½½ PDF æ–‡æ¡£...")
    loader = DirectoryLoader(DATA_PATH, glob="*.pdf", loader_cls=PyPDFLoader)
    documents = loader.load()
    if not documents:
        print("âŒ é”™è¯¯ï¼šdata æ–‡ä»¶å¤¹é‡Œæ²¡æœ‰ PDFï¼")
        return None
    print(f"   âœ… æˆåŠŸåŠ è½½ {len(documents)} é¡µæ–‡æ¡£ã€‚")

    # --- 4. åˆ‡åˆ† ---
    print("ğŸ”„ 2. æ­£åœ¨åˆ‡åˆ†æ–‡æœ¬...")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_documents(documents)
    print(f"   âœ… æˆåŠŸåˆ‡åˆ†ä¸º {len(chunks)} ä¸ªæ–‡æœ¬å—ã€‚")

    # --- 5. å‘é‡åŒ–å¹¶å­˜å‚¨ ---
    print("ğŸ”„ 3. æ­£åœ¨å†™å…¥æ–°æ•°æ®åº“...")
    # è¿™é‡Œä¸éœ€è¦å†åˆ é™¤äº†ï¼Œå› ä¸ºå¼€å¤´å·²ç»åˆ è¿‡äº†
    db = Chroma.from_documents(chunks, embeddings, persist_directory=CHROMA_PATH)
    print(f"   âœ… å‘é‡æ•°æ®åº“å·²é‡å»ºå®Œæˆï¼")
    return db


def search_rag(query_text):
    """
    æµ‹è¯•æ£€ç´¢åŠŸèƒ½ï¼šè¾“å…¥é—®é¢˜ -> è¿”å›æœ€ç›¸å…³çš„ç‰‡æ®µ
    """
    # é‡æ–°åŠ è½½å·²ç»ä¿å­˜çš„æ•°æ®åº“
    embeddings = HuggingFaceEmbeddings(
        model_name="model/m3e-base",
        model_kwargs={'device': 'mps'}  # å…³é”®ï¼šè¿™è¡Œä»£ç èƒ½è°ƒç”¨ä½  Mac çš„ GPU/NPU åŠ é€Ÿ
    )
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)

    print(f"\nğŸ” æ­£åœ¨æ£€ç´¢é—®é¢˜ï¼š{query_text}")
    # k=3 è¡¨ç¤ºè¿”å›æœ€ç›¸å…³çš„ 3 ä¸ªç‰‡æ®µ
    results = db.similarity_search(query_text, k=3)

    return results

def get_rag_response(question):
    """
    æ ¸å¿ƒé€»è¾‘ï¼šæ£€ç´¢ + ç”Ÿæˆ
    """
    # 1. è¿æ¥æ•°æ®åº“
    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)

    # 2. æ£€ç´¢ (Retrieve) - æ‰¾ 3 ä¸ªç›¸å…³ç‰‡æ®µ
    docs = db.similarity_search(question, k=3)

    # 3. æ‹¼æ¥ä¸Šä¸‹æ–‡ (Context)
    context_text = "\n\n".join([doc.page_content for doc in docs])

    # 4. æ„é€  Prompt (è¿™å°±æ˜¯ Prompt Engineeringï¼)
    # æˆ‘ä»¬å¼ºåˆ¶è¦æ±‚å®ƒæ‰®æ¼” IC ä¸“å®¶ï¼Œå¹¶ä¸”åªèƒ½åŸºäº Context å›ç­”
    PROMPT_TEMPLATE = """
    ä½ æ˜¯ä¸€åé›†æˆç”µè·¯(IC)é¢†åŸŸçš„èµ„æ·±æŠ€æœ¯ä¸“å®¶ã€‚è¯·åŸºäºä¸‹é¢çš„ã€å‚è€ƒèµ„æ–™ã€‘å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

    è§„åˆ™ï¼š
    1. å¦‚æœå‚è€ƒèµ„æ–™é‡Œæœ‰ç­”æ¡ˆï¼Œè¯·ç”¨ä¸“ä¸šã€ç®€æ´çš„è¯­è¨€å›ç­”ã€‚
    2. å¦‚æœå‚è€ƒèµ„æ–™é‡Œæ²¡æœ‰ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´â€œçŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯â€ï¼Œä¸è¦çç¼–ã€‚
    3. å¦‚æœæ¶‰åŠ Verilog ä»£ç ï¼Œè¯·ç¡®ä¿è¯­æ³•æ­£ç¡®ã€‚

    ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
    {context}

    ã€ç”¨æˆ·é—®é¢˜ã€‘ï¼š
    {question}
    """

    prompt = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
    prompt_text = prompt.format(context=context_text, question=question)

    # 5. ç”Ÿæˆ (Generate)
    print(f"ğŸ˜‰ æ­£åœ¨æ€è€ƒ... (Prompt é•¿åº¦: {len(prompt_text)})")
    response = llm.invoke(prompt_text)

    # è¿”å›ï¼šç”Ÿæˆçš„å›ç­” + å¼•ç”¨æ¥æº
    return response.content, docs


# æµ‹è¯•ä»£ç å…¥å£
if __name__ == "__main__":
    # ç¬¬ä¸€æ­¥ï¼šå¼ºåˆ¶é‡æ–°å­¦ä¹ ï¼(è¿™ä¸€æ­¥å°±æ˜¯ä½ åœ¨å›¾ç‰‡é‡Œçœ‹åˆ°çš„é€»è¾‘)
    # å®ƒä¼šæ¸…ç©ºæ—§çš„ chroma_dbï¼ŒæŠŠ data æ–‡ä»¶å¤¹é‡Œçš„æ–° PDF é‡æ–°åƒè¿›å»
    create_vector_db()

    # ç¬¬äºŒæ­¥ï¼šå­¦ä¹ å®Œäº†ï¼Œç°åœ¨å¼€å§‹æé—®
    question = "ä»€ä¹ˆæ˜¯ Verilogï¼Ÿ"
    answer, sources = get_rag_response(question)

    print(f"\nâ“ é—®é¢˜: {question}")
    print("-" * 50)
    print(f"ğŸ˜ AI å›ç­”: {answer}")
    print("-" * 50)
    print("ğŸ“š å‚è€ƒæ¥æº:")
    for doc in sources:
        print(f" - {doc.metadata.get('source')} (å†…å®¹ç‰‡æ®µ: {doc.page_content[:20]}...)")